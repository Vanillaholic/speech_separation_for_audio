import torch
import numpy as np
from pyannote.audio import Pipeline
from pyannote.core import Segment
import soundfile as sf
from pathlib import Path
import os
import tempfile
import json
import gradio as gr

# è§†é¢‘å¤„ç†
MOVIEPY_AVAILABLE = False
try:
    from moviepy.editor import VideoFileClip
    MOVIEPY_AVAILABLE = True
except ImportError as e:
    # ImportError: æ¨¡å—æœªå®‰è£…æˆ–æ‰¾ä¸åˆ°
    error_msg = str(e)
    if "No module named 'moviepy'" in error_msg or "No module named 'moviepy.editor'" in error_msg:
        pass  # åªåœ¨çœŸæ­£ç¼ºå°‘æ¨¡å—æ—¶æ‰“å°ï¼Œå¦åˆ™ä¸æ‰“å°ï¼ˆå¯èƒ½åœ¨è¿è¡Œæ—¶å¯ç”¨ï¼‰
except Exception:
    pass  # é™é»˜å¤„ç†å…¶ä»–å¼‚å¸¸ï¼Œåœ¨è¿è¡Œæ—¶å†æ£€æŸ¥

# åœ¨è¿è¡Œæ—¶æ£€æŸ¥ moviepy æ˜¯å¦å¯ç”¨
def check_moviepy_available():
    """è¿è¡Œæ—¶æ£€æŸ¥ moviepy æ˜¯å¦å¯ç”¨"""
    try:
        from moviepy.editor import VideoFileClip
        return True
    except:
        return False

class SpeakerSeparator:
    """éŸ³é¢‘è¯´è¯äººåˆ†ç¦»å™¨ï¼Œæ”¯æŒä¸¤ç§è¾“å‡ºæ¨¡å¼"""
    
    def __init__(self, config_file="config.json"):
        self.pipeline_cache = None
        self.cached_token = None
        self.config_file = config_file
        self.hf_token = self.load_token_from_config()
    
    def load_token_from_config(self):
        """ä»é…ç½®æ–‡ä»¶è¯»å– token"""
        try:
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    token = config.get('hf_token', '')
                    if token and token != "è¯·è¾“å…¥ä½ çš„ Hugging Face Token":
                        return token
            print(f"è­¦å‘Š: æœªæ‰¾åˆ°æœ‰æ•ˆçš„ tokenï¼Œè¯·æ£€æŸ¥ {self.config_file}")
            return None
        except Exception as e:
            print(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def load_pipeline(self, hf_token):
        """åŠ è½½æˆ–è·å–ç¼“å­˜çš„pipeline"""
        if self.pipeline_cache is None or self.cached_token != hf_token:
            if not hf_token:
                raise ValueError("è¯·è¾“å…¥æœ‰æ•ˆçš„Hugging Face Token")
            
            os.environ["HF_TOKEN"] = hf_token
            
            try:
                self.pipeline_cache = Pipeline.from_pretrained(
                    "pyannote/speaker-diarization-3.1",
                    token=hf_token
                )
            except Exception as e:
                try:
                    self.pipeline_cache = Pipeline.from_pretrained(
                        "pyannote/speaker-diarization-3.1",
                        use_auth_token=hf_token
                    )
                except Exception as e2:
                    try:
                        self.pipeline_cache = Pipeline.from_pretrained(
                            "pyannote/speaker-diarization-3.1"
                        )
                    except Exception as e3:
                        raise ValueError(
                            f"æ— æ³•åŠ è½½æ¨¡å‹: {e3}\n"
                            "è¯·æ£€æŸ¥:\n"
                            "1. Hugging Face Token æ˜¯å¦æ­£ç¡®\n"
                            "2. æ˜¯å¦å·²åœ¨ Hugging Face ä¸ŠåŒæ„ pyannote/speaker-diarization-3.1 çš„ç”¨æˆ·åè®®\n"
                            "3. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸"
                        )
            
            self.cached_token = hf_token
        
        return self.pipeline_cache
    
    def separate_speakers(
        self,
        audio_file,
        num_speakers,
        onset_threshold,
        offset_threshold,
        min_duration_on,
        min_duration_off,
        clustering_threshold,
        output_mode1,
        output_mode2
    ):
        """
        æ‰§è¡Œè¯´è¯äººåˆ†ç¦»
        
        å‚æ•°:
            audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆGradioä¸Šä¼ çš„æ–‡ä»¶ï¼‰
            hf_token: Hugging Face token
            num_speakers: è¯´è¯äººæ•°é‡
            onset_threshold: è¯­éŸ³å¼€å§‹é˜ˆå€¼
            offset_threshold: è¯­éŸ³ç»“æŸé˜ˆå€¼
            min_duration_on: æœ€å°è¯­éŸ³æ—¶é•¿
            min_duration_off: æœ€å°é™éŸ³æ—¶é•¿
            clustering_threshold: èšç±»é˜ˆå€¼
            output_mode1: æ˜¯å¦è¾“å‡ºæ¨¡å¼1ï¼ˆæ—¶é•¿=åŸéŸ³é¢‘ï¼Œéè¯´è¯äººæ—¶æ®µä¸ºé™éŸ³ï¼‰
            output_mode2: æ˜¯å¦è¾“å‡ºæ¨¡å¼2ï¼ˆåªåŒ…å«è¯´è¯æ—¶æ®µï¼Œæ— é™éŸ³ï¼‰
        
        è¿”å›:
            è¾“å‡ºæ–‡ä»¶åˆ—è¡¨å’Œæ¶ˆæ¯
        """
        
        if audio_file is None:
            return [], [], "è¯·ä¸Šä¼ éŸ³é¢‘æˆ–è§†é¢‘æ–‡ä»¶"
        
        # ä½¿ç”¨é…ç½®ä¸­çš„ token
        hf_token = self.hf_token
        if not hf_token:
            return [], [], f"é”™è¯¯: æœªæ‰¾åˆ°æœ‰æ•ˆçš„ Hugging Face Tokenï¼Œè¯·æ£€æŸ¥ {self.config_file} é…ç½®æ–‡ä»¶"
        
        # åˆå§‹åŒ–æ¶ˆæ¯
        message = ""
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹ï¼Œå¦‚æœæ˜¯è§†é¢‘åˆ™å…ˆæå–éŸ³é¢‘
        file_ext = Path(audio_file).suffix.lower()
        is_video = file_ext in ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.webm', '.m4v']
        
        actual_audio_file = audio_file
        temp_audio_path = None
        
        if is_video:
            # è¿è¡Œæ—¶å†æ¬¡æ£€æŸ¥ moviepy æ˜¯å¦å¯ç”¨
            try:
                from moviepy.editor import VideoFileClip
            except ImportError as e:
                return [], [], f"é”™è¯¯: æ— æ³•å¤„ç†è§†é¢‘æ–‡ä»¶ã€‚moviepy å¯¼å…¥å¤±è´¥: {e}\nè¯·æ£€æŸ¥ moviepy æ˜¯å¦æ­£ç¡®å®‰è£…: pip install moviepy"
            except Exception as e:
                return [], [], f"é”™è¯¯: æ— æ³•å¤„ç†è§†é¢‘æ–‡ä»¶ã€‚moviepy å¯¼å…¥æ—¶å‡ºç°å¼‚å¸¸: {type(e).__name__}: {e}\nå¦‚æœå·²å®‰è£… moviepyï¼Œå¯èƒ½æ˜¯ä¾èµ–é—®é¢˜ï¼Œè¯·æ£€æŸ¥ imageio-ffmpeg: pip install imageio-ffmpeg"
            
            try:
                message += "æ£€æµ‹åˆ°è§†é¢‘æ–‡ä»¶ï¼Œæ­£åœ¨æå–éŸ³é¢‘...\n"
                # ä»è§†é¢‘æå–éŸ³é¢‘
                with VideoFileClip(audio_file) as video:
                    audio = video.audio
                    if audio is None:
                        return [], [], "é”™è¯¯: è§†é¢‘æ–‡ä»¶ä¸­æ²¡æœ‰éŸ³é¢‘è½¨é“"
                    
                    # åˆ›å»ºä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
                    temp_audio_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
                    temp_audio_path = temp_audio_file.name
                    temp_audio_file.close()
                    
                    # å†™å…¥éŸ³é¢‘
                    audio.write_audiofile(temp_audio_path, verbose=False, logger=None)
                    actual_audio_file = temp_audio_path
                    message += f"éŸ³é¢‘æå–å®Œæˆï¼Œé‡‡æ ·ç‡: {audio.fps}Hz\n"
                    # å…³é—­éŸ³é¢‘å¯¹è±¡ä»¥é‡Šæ”¾èµ„æº
                    audio.close()
            except Exception as e:
                return [], [], f"ä»è§†é¢‘æå–éŸ³é¢‘å¤±è´¥: {e}"
        
        # è¯»å–éŸ³é¢‘æ–‡ä»¶
        try:
            audio_data, sample_rate = sf.read(actual_audio_file)
        except Exception as e:
            return [], [], f"è¯»å–éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {e}"
        finally:
            # æ¸…ç†ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
            if temp_audio_path and os.path.exists(temp_audio_path):
                try:
                    os.unlink(temp_audio_path)
                except:
                    pass
        
        # ç¡®ä¿éŸ³é¢‘æ˜¯å•å£°é“
        if len(audio_data.shape) > 1:
            audio_data = np.mean(audio_data, axis=1)
        
        # è½¬æ¢ä¸ºfloat32
        audio_data = audio_data.astype(np.float32)
        
        # åŠ è½½pipeline
        try:
            pipeline = self.load_pipeline(hf_token)
        except Exception as e:
            return [], [], str(e)
        
        # é…ç½®pipelineå‚æ•°
        try:
            default_params = pipeline.parameters(instantiated=True)
            valid_params = {}
            
            param_map = {
                'onset': onset_threshold,
                'offset': offset_threshold,
                'min_duration_on': min_duration_on,
                'min_duration_off': min_duration_off,
            }
            
            for param_name, param_value in param_map.items():
                if param_name in default_params:
                    valid_params[param_name] = param_value
            
            # å°è¯•è®¾ç½®èšç±»å‚æ•°
            if 'clustering' in default_params:
                if isinstance(default_params['clustering'], dict):
                    clustering_params = default_params['clustering'].copy()
                    clustering_params['threshold'] = clustering_threshold
                    valid_params['clustering'] = clustering_params
            
            if valid_params:
                pipeline.instantiate(valid_params)
        except Exception as e:
            print(f"å‚æ•°è®¾ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°: {e}")
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºpipelineå¤„ç†
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_file:
            tmp_path = tmp_file.name
            sf.write(tmp_path, audio_data, sample_rate)
        
        output_files = []
        audio_list = []
        
        # message å·²ç»åœ¨å‰é¢åˆå§‹åŒ–äº†ï¼Œå¦‚æœæœ‰è§†é¢‘å¤„ç†åˆ™å·²æœ‰å†…å®¹
        
        try:
            # æ‰§è¡Œè¯´è¯äººåˆ†ç¦»
            diarization = pipeline(tmp_path, num_speakers=num_speakers)
            
            # æ”¶é›†æ‰€æœ‰è¯´è¯äººçš„æ—¶é—´æ®µ
            speaker_segments = {}
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                if speaker not in speaker_segments:
                    speaker_segments[speaker] = []
                speaker_segments[speaker].append((turn.start, turn.end))
            
            if not speaker_segments:
                message = "æœªæ£€æµ‹åˆ°ä»»ä½•è¯´è¯äººï¼"
                return output_files, [], message
            
            # æŒ‰è¯´è¯äººIDæ’åº
            sorted_speakers = sorted(speaker_segments.keys())
            num_detected = len(sorted_speakers)
            message += f"æ£€æµ‹åˆ° {num_detected} ä¸ªè¯´è¯äºº\n"
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = Path('speaker_output')
            output_dir.mkdir(exist_ok=True)
            
            # æ¨¡å¼1ï¼šæ—¶é•¿ç­‰äºåŸéŸ³é¢‘ï¼Œéè¯´è¯äººæ—¶æ®µä¸ºé™éŸ³
            if output_mode1:
                message += "\næ¨¡å¼1è¾“å‡ºï¼ˆæ—¶é•¿=åŸéŸ³é¢‘ï¼Œéè¯´è¯äººæ—¶æ®µä¸ºé™éŸ³ï¼‰ï¼š\n"
                for speaker_id in sorted_speakers:
                    segments = speaker_segments[speaker_id]
                    
                    # åˆ›å»ºå…¨é›¶æ•°ç»„ï¼ˆé™éŸ³ï¼‰ï¼Œé•¿åº¦ä¸åŸå§‹éŸ³é¢‘ç›¸åŒ
                    speaker_audio = np.zeros_like(audio_data)
                    
                    # å°†è¯´è¯äººçš„è¯­éŸ³æ®µå¤åˆ¶åˆ°å¯¹åº”ä½ç½®
                    for start_time, end_time in segments:
                        start_sample = int(start_time * sample_rate)
                        end_sample = int(end_time * sample_rate)
                        
                        # ç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
                        start_sample = min(start_sample, len(audio_data))
                        end_sample = min(end_sample, len(audio_data))
                        
                        if start_sample < end_sample:
                            speaker_audio[start_sample:end_sample] = audio_data[start_sample:end_sample]
                    
                    # ä¿å­˜æ–‡ä»¶
                    output_file = output_dir / f"mode1_speaker_{speaker_id}.wav"
                    sf.write(str(output_file), speaker_audio, sample_rate)
                    output_files.append(str(output_file))
                    message += f"  - {output_file.name}\n"
            
            # æ¨¡å¼2ï¼šåªåŒ…å«è¯´è¯æ—¶æ®µï¼Œæ— é™éŸ³ï¼Œæ—¶é•¿ä¸ç­‰äºåŸéŸ³é¢‘
            if output_mode2:
                message += "\næ¨¡å¼2è¾“å‡ºï¼ˆåªåŒ…å«è¯´è¯æ—¶æ®µï¼Œæ— é™éŸ³ï¼‰ï¼š\n"
                for speaker_id in sorted_speakers:
                    segments = speaker_segments[speaker_id]
                    
                    # æ”¶é›†æ‰€æœ‰è¯¥è¯´è¯äººçš„éŸ³é¢‘ç‰‡æ®µ
                    speaker_chunks = []
                    for start_time, end_time in segments:
                        start_sample = int(start_time * sample_rate)
                        end_sample = int(end_time * sample_rate)
                        
                        # ç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
                        start_sample = min(start_sample, len(audio_data))
                        end_sample = min(end_sample, len(audio_data))
                        
                        if start_sample < end_sample:
                            chunk = audio_data[start_sample:end_sample]
                            speaker_chunks.append(chunk)
                    
                    # å¦‚æœæœ‰éŸ³é¢‘ç‰‡æ®µï¼Œæ‹¼æ¥å®ƒä»¬
                    if speaker_chunks:
                        speaker_audio = np.concatenate(speaker_chunks)
                        
                        # ä¿å­˜æ–‡ä»¶
                        output_file = output_dir / f"mode2_speaker_{speaker_id}.wav"
                        sf.write(str(output_file), speaker_audio, sample_rate)
                        output_files.append(str(output_file))
                        message += f"  - {output_file.name}\n"
            
            message += f"\nå¤„ç†å®Œæˆï¼å…±ç”Ÿæˆ {len(output_files)} ä¸ªæ–‡ä»¶ã€‚"
            
        except Exception as e:
            message = f"å¤„ç†å¤±è´¥: {str(e)}"
        
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(tmp_path)
            except:
                pass
        
        # è¿”å›æ–‡ä»¶åˆ—è¡¨å’Œæ¶ˆæ¯
        # output_files æ˜¯æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œç”¨äºæ˜¾ç¤ºå’Œä¸‹è½½
        return output_files, output_files, message


# åˆ›å»ºåˆ†ç¦»å™¨å®ä¾‹
separator = SpeakerSeparator()

# åˆ›å»ºGradioç•Œé¢
def create_interface():
    with gr.Blocks(title="éŸ³è§†é¢‘è¯´è¯äººåˆ†ç¦»å·¥å…·", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ¤ éŸ³è§†é¢‘è¯´è¯äººåˆ†ç¦»å·¥å…· | authorized by Zihan Xing")
        gr.Markdown("ä¸Šä¼ éŸ³é¢‘æˆ–è§†é¢‘æ–‡ä»¶ï¼Œè‡ªåŠ¨åˆ†ç¦»ä¸åŒçš„è¯´è¯äººã€‚æ”¯æŒä¸¤ç§è¾“å‡ºæ¨¡å¼ï¼š")
        gr.Markdown("- **æ¨¡å¼1**ï¼šæ¯ä¸ªè¯´è¯äººçš„éŸ³é¢‘æ—¶é•¿ç­‰äºåŸéŸ³é¢‘ï¼Œéè¯´è¯äººæ—¶æ®µä¸ºé™éŸ³")
        gr.Markdown("- **æ¨¡å¼2**ï¼šæ¯ä¸ªè¯´è¯äººçš„éŸ³é¢‘åªåŒ…å«è¯´è¯æ—¶æ®µï¼Œæ— é™éŸ³ï¼Œæ—¶é•¿ä¸ç­‰äºåŸéŸ³é¢‘")
        
        with gr.Row():
            with gr.Column():
                audio_input = gr.File(
                    label="ä¸Šä¼ éŸ³é¢‘æˆ–è§†é¢‘æ–‡ä»¶",
                    file_types=["audio", "video"]
                )
                
                # æ·»åŠ ä¸€ä¸ªéŸ³é¢‘é¢„è§ˆç»„ä»¶ï¼ˆä»…ç”¨äºé¢„è§ˆä¸Šä¼ çš„æ–‡ä»¶ï¼‰
                audio_preview = gr.Audio(
                    label="éŸ³é¢‘é¢„è§ˆï¼ˆä¸Šä¼ åè‡ªåŠ¨æ˜¾ç¤ºï¼‰",
                    type="filepath",
                    interactive=False
                )
                
                with gr.Accordion("åˆ†ç¦»å‚æ•°è®¾ç½®", open=False):
                    num_speakers = gr.Slider(
                        label="è¯´è¯äººæ•°é‡",
                        minimum=1,
                        maximum=10,
                        value=2,
                        step=1,
                        info="é¢„æœŸçš„è¯´è¯äººæ•°é‡"
                    )
                    
                    onset_threshold = gr.Slider(
                        label="Onseté˜ˆå€¼",
                        minimum=0.0,
                        maximum=1.0,
                        value=0.7,
                        step=0.01,
                        info="è¯­éŸ³å¼€å§‹æ£€æµ‹çš„æ•æ„Ÿåº¦ï¼Œå€¼è¶Šå¤§è¶Šä¸¥æ ¼"
                    )
                    
                    offset_threshold = gr.Slider(
                        label="Offseté˜ˆå€¼",
                        minimum=0.0,
                        maximum=1.0,
                        value=0.7,
                        step=0.01,
                        info="è¯­éŸ³ç»“æŸæ£€æµ‹çš„æ•æ„Ÿåº¦ï¼Œå€¼è¶Šå¤§è¶Šä¸¥æ ¼"
                    )
                    
                    min_duration_on = gr.Slider(
                        label="æœ€å°è¯­éŸ³æ—¶é•¿(ç§’)",
                        minimum=0.0,
                        maximum=5.0,
                        value=0.1,
                        step=0.01,
                        info="è¿‡æ»¤è¿‡çŸ­çš„è¯­éŸ³ç‰‡æ®µ"
                    )
                    
                    min_duration_off = gr.Slider(
                        label="æœ€å°é™éŸ³æ—¶é•¿(ç§’)",
                        minimum=0.0,
                        maximum=5.0,
                        value=0.1,
                        step=0.01,
                        info="è¿‡æ»¤è¿‡çŸ­çš„é™éŸ³ç‰‡æ®µ"
                    )
                    
                    clustering_threshold = gr.Slider(
                        label="èšç±»é˜ˆå€¼",
                        minimum=0.0,
                        maximum=1.0,
                        value=0.7,
                        step=0.01,
                        info="è¯´è¯äººèšç±»çš„é˜ˆå€¼ï¼Œå½±å“è¯´è¯äººåˆ†ç»„"
                    )
                
                with gr.Accordion("è¾“å‡ºæ¨¡å¼é€‰æ‹©", open=True):
                    output_mode1 = gr.Checkbox(
                        label="æ¨¡å¼1ï¼šæ—¶é•¿ç­‰äºåŸéŸ³é¢‘ï¼ˆéè¯´è¯äººæ—¶æ®µä¸ºé™éŸ³ï¼‰",
                        value=True,
                        info="æ¯ä¸ªè¯´è¯äººçš„éŸ³é¢‘æ—¶é•¿ç­‰äºåŸéŸ³é¢‘æ—¶é•¿"
                    )
                    
                    output_mode2 = gr.Checkbox(
                        label="æ¨¡å¼2ï¼šåªåŒ…å«è¯´è¯æ—¶æ®µï¼ˆæ— é™éŸ³ï¼‰",
                        value=False,
                        info="æ¯ä¸ªè¯´è¯äººçš„éŸ³é¢‘åªåŒ…å«è¯´è¯æ—¶æ®µï¼Œæ—¶é•¿ä¸ç­‰äºåŸéŸ³é¢‘"
                    )
                    
                    gr.Markdown("âš ï¸ è‡³å°‘é€‰æ‹©ä¸€ç§è¾“å‡ºæ¨¡å¼")
                
                process_btn = gr.Button("å¼€å§‹åˆ†ç¦»", variant="primary", size="lg")
            
            with gr.Column():
                message_output = gr.Textbox(
                    label="å¤„ç†ä¿¡æ¯",
                    lines=10,
                    interactive=False
                )
                
                # è¾“å‡ºéŸ³é¢‘é¢„è§ˆåŒºåŸŸ
                gr.Markdown("### è¾“å‡ºéŸ³é¢‘æ–‡ä»¶ï¼ˆå¯é¢„è§ˆå’Œä¸‹è½½ï¼‰")
                
                with gr.Row():
                    output_audio_1 = gr.Audio(label="è¯´è¯äºº 1", type="filepath")
                    output_audio_2 = gr.Audio(label="è¯´è¯äºº 2", type="filepath")
                
                with gr.Row():
                    output_audio_3 = gr.Audio(label="è¯´è¯äºº 3", type="filepath")
                    output_audio_4 = gr.Audio(label="è¯´è¯äºº 4", type="filepath")
                
                output_audio_5 = gr.Audio(label="è¯´è¯äºº 5", type="filepath")
        
        # å¤„ç†å‡½æ•°
        def process_audio(
            audio_file,
            num_speakers,
            onset_threshold,
            offset_threshold,
            min_duration_on,
            min_duration_off,
            clustering_threshold,
            output_mode1,
            output_mode2
        ):
            if not output_mode1 and not output_mode2:
                return (
                    "è¯·è‡³å°‘é€‰æ‹©ä¸€ç§è¾“å‡ºæ¨¡å¼ï¼",
                    None,  # é¢„è§ˆéŸ³é¢‘
                    None,
                    None,
                    None,
                    None,
                    None
                )
            
            # gr.File è¿”å›æ–‡ä»¶è·¯å¾„ï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–æ–‡ä»¶å¯¹è±¡ï¼‰
            audio_path = None
            preview_audio_path = None
            
            if audio_file is not None:
                if isinstance(audio_file, (list, tuple)) and len(audio_file) > 0:
                    # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ª
                    audio_path = audio_file[0].name if hasattr(audio_file[0], 'name') else str(audio_file[0])
                elif hasattr(audio_file, 'name'):
                    # æ–‡ä»¶å¯¹è±¡æœ‰ name å±æ€§
                    audio_path = audio_file.name
                else:
                    # ç›´æ¥æ˜¯å­—ç¬¦ä¸²è·¯å¾„
                    audio_path = str(audio_file)
                
                # å¦‚æœæ˜¯è§†é¢‘æ–‡ä»¶ï¼Œå…ˆæå–éŸ³é¢‘ç”¨äºé¢„è§ˆ
                file_ext = Path(audio_path).suffix.lower() if audio_path else ""
                is_video = file_ext in ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.webm', '.m4v']
                
                if is_video:
                    # å¯¹äºè§†é¢‘ï¼Œæå–éŸ³é¢‘ç”¨äºé¢„è§ˆ
                    if check_moviepy_available():
                        try:
                            from moviepy.editor import VideoFileClip
                            with VideoFileClip(audio_path) as video:
                                if video.audio:
                                    temp_preview = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
                                    temp_preview_path = temp_preview.name
                                    temp_preview.close()
                                    video.audio.write_audiofile(temp_preview_path, verbose=False, logger=None)
                                    preview_audio_path = temp_preview_path
                                    video.audio.close()
                        except Exception as e:
                            print(f"è§†é¢‘é¢„è§ˆå¤±è´¥: {e}")
                else:
                    # å¦‚æœæ˜¯éŸ³é¢‘æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨
                    preview_audio_path = audio_path
            
            files, audio_list, message = separator.separate_speakers(
                audio_file=audio_path,
                num_speakers=int(num_speakers),
                onset_threshold=onset_threshold,
                offset_threshold=offset_threshold,
                min_duration_on=min_duration_on,
                min_duration_off=min_duration_off,
                clustering_threshold=clustering_threshold,
                output_mode1=output_mode1,
                output_mode2=output_mode2
            )
            
            # å°†æ–‡ä»¶åˆ—è¡¨æ·»åŠ åˆ°æ¶ˆæ¯ä¸­
            if files:
                message += "\n\n" + "="*50 + "\n"
                message += "ç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨ï¼š\n"
                message += "="*50 + "\n"
                for i, file_path in enumerate(files, 1):
                    message += f"{i}. {file_path}\n"
            
            # å‡†å¤‡è¿”å›çš„éŸ³é¢‘æ–‡ä»¶ï¼ˆæœ€å¤š5ä¸ªï¼‰
            audio_outputs = [None] * 5
            for i, file_path in enumerate(files[:5]):
                audio_outputs[i] = file_path
            
            # è¿”å›æ¶ˆæ¯ã€é¢„è§ˆéŸ³é¢‘å’Œè¾“å‡ºéŸ³é¢‘æ–‡ä»¶ï¼ˆæ˜¾ç¤ºå‰5ä¸ªï¼‰
            # åœ¨ Gradio 4.19.0 ä¸­ç›´æ¥è¿”å›å€¼ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ update() æ–¹æ³•
            return (
                message,
                preview_audio_path,  # æ·»åŠ é¢„è§ˆéŸ³é¢‘
                audio_outputs[0],
                audio_outputs[1],
                audio_outputs[2],
                audio_outputs[3],
                audio_outputs[4]
            )
        
        # æ–‡ä»¶ä¸Šä¼ åçš„é¢„è§ˆå‡½æ•°
        def preview_uploaded_file(audio_file):
            """å½“æ–‡ä»¶ä¸Šä¼ åï¼Œè‡ªåŠ¨æ˜¾ç¤ºé¢„è§ˆ"""
            if audio_file is None:
                return None
            
            # è·å–æ–‡ä»¶è·¯å¾„
            if isinstance(audio_file, (list, tuple)) and len(audio_file) > 0:
                audio_path = audio_file[0].name if hasattr(audio_file[0], 'name') else str(audio_file[0])
            elif hasattr(audio_file, 'name'):
                audio_path = audio_file.name
            else:
                audio_path = str(audio_file)
            
            if not audio_path:
                return None
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶
            file_ext = Path(audio_path).suffix.lower()
            is_video = file_ext in ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.webm', '.m4v']
            
            if is_video:
                # å¦‚æœæ˜¯è§†é¢‘ï¼Œæå–éŸ³é¢‘ç”¨äºé¢„è§ˆ
                if not check_moviepy_available():
                    # ä¸æ‰“å°è­¦å‘Šï¼Œåªæ˜¯è¿”å› None
                    return None
                try:
                    from moviepy.editor import VideoFileClip
                    with VideoFileClip(audio_path) as video:
                        if video.audio:
                            temp_preview = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
                            temp_preview_path = temp_preview.name
                            temp_preview.close()
                            video.audio.write_audiofile(temp_preview_path, verbose=False, logger=None)
                            video.audio.close()
                            return temp_preview_path
                except Exception as e:
                    print(f"è§†é¢‘é¢„è§ˆå¤±è´¥: {e}")
                    return None
            else:
                # å¦‚æœæ˜¯éŸ³é¢‘æ–‡ä»¶ï¼Œç›´æ¥è¿”å›è·¯å¾„
                return audio_path
        
        # ç»‘å®šæ–‡ä»¶ä¸Šä¼ äº‹ä»¶ï¼ˆè‡ªåŠ¨é¢„è§ˆï¼‰
        audio_input.change(
            fn=preview_uploaded_file,
            inputs=[audio_input],
            outputs=[audio_preview]
        )
        
        # ç»‘å®šå¤„ç†äº‹ä»¶
        process_btn.click(
            fn=process_audio,
            inputs=[
                audio_input,
                num_speakers,
                onset_threshold,
                offset_threshold,
                min_duration_on,
                min_duration_off,
                clustering_threshold,
                output_mode1,
                output_mode2
            ],
            outputs=[
                message_output,
                audio_preview,
                output_audio_1,
                output_audio_2,
                output_audio_3,
                output_audio_4,
                output_audio_5
            ]
        )
    
    return demo


if __name__ == "__main__":
    import socket
    import os
    
    # ç¦ç”¨ Gradio çš„ analytics ä»¥é¿å…ç½‘ç»œè¶…æ—¶é”™è¯¯
    os.environ["GRADIO_ANALYTICS_ENABLED"] = "False"
    
    def find_free_port(start_port=7860, max_attempts=10):
        """æŸ¥æ‰¾å¯ç”¨ç«¯å£"""
        for i in range(max_attempts):
            port = start_port + i
            try:
                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                    s.bind(('', port))
                    return port
            except OSError:
                continue
        return None
    
    demo = create_interface()
    
    # å°è¯•æ‰¾åˆ°å¯ç”¨ç«¯å£
    port = find_free_port(7860)
    if port:
        print(f"æ­£åœ¨å¯åŠ¨æœåŠ¡ï¼Œç«¯å£: {port}")
        try:
            demo.launch(server_port=port, share=False, show_error=True)
        except OSError as e:
            if "Cannot find empty port" in str(e):
                print(f"ç«¯å£ {port} ä¸å¯ç”¨ï¼Œå°è¯•è‡ªåŠ¨é€‰æ‹©ç«¯å£...")
                demo.launch(share=False, show_error=True, server_port=None)
            else:
                raise
    else:
        print("æ­£åœ¨å¯åŠ¨æœåŠ¡ï¼ˆè‡ªåŠ¨é€‰æ‹©ç«¯å£ï¼‰")
        demo.launch(share=False, show_error=True, server_port=None)


